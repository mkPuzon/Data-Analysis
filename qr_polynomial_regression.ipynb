{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME HERE**\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "CS 252: Mathematical Data Analysis and Visualization\n",
    "\n",
    "Project 2: Linear regression\n",
    "\n",
    "#### Week 2: QR-based linear regression and polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "import linear_regression\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- In your implementations, only the following \"high level\" `scipy`/`numpy` functions can be used:\n",
    "    - `np.linalg.inv`\n",
    "    - `scipy.linalg.lstsq` (in `LinearRegression::linear_regression_scipy` only).\n",
    "    - `np.linalg.norm`\n",
    "    - `scipy.linalg.solve_triangular`, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement a QR based linear regression solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. QR-decomposition\n",
    "\n",
    "- Write algorithm to compute QR decomposition (`linear_regression::qr_decomposition`). **Run test code below.** Equation for R: $$R = Q^TA$$\n",
    "- Implement `linear_regression::linear_regression_qr` to use the QR decomposition to do the linear regression. Recall that the equation is $$Rc = Q^Ty$$ which can be solved without taking the inverse of $R$ through backsolving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test QR decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_filename = 'data/iris.csv'\n",
    "iris_data = data.Data(iris_filename)\n",
    "\n",
    "A = iris_data.select_data(['sepal_length', 'petal_width'])\n",
    "A1 = np.hstack([A, np.ones([len(A), 1])])\n",
    "\n",
    "lin_reg_qr = linear_regression.LinearRegression(iris_data)\n",
    "myQ, myR = lin_reg_qr.qr_decomposition(A1)\n",
    "\n",
    "Q, R = np.linalg.qr(A1)\n",
    "\n",
    "print(f'Your Q shape is {myQ.shape} and should be {Q.shape}')\n",
    "print(f'Your R shape is {myR.shape} and should be {R.shape}')\n",
    "print(f'1st few rows of your Q are\\n{myQ[:3]} and should be\\n{Q[:3]}')\n",
    "print('NOTE: It is fine if your Q numbers match in absolute value\\nbut 1 or more COLUMNS are negated.')\n",
    "print(f'\\nYour R is\\n{myR} and should be\\n{R}')\n",
    "print('NOTE: It is fine if your R numbers match in absolute value\\nbut 1 or more ROWS are negated.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test linear regression via QR decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_qr.linear_regression(['sepal_length'], 'petal_width', 'qr')\n",
    "lin_reg_qr.scatter('sepal_length', 'petal_width', 'qr')\n",
    "lin_reg_qr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** How do your results compare to the built-in SciPy solver? Is this what you expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b. Compare performance of the linear regression methods\n",
    "\n",
    "- In the cell below, load in the brain network `brain.csv` dataset.\n",
    "- Create `Data` and `LinearRegression` objects.\n",
    "- Load in the list of variable names in the brain dataset (62)\n",
    "- Do multiple linear regressions, separately for each of the linear regression methods that you have implemented (including `scipy`): Set all brain data variables except for the last one as the independent variables, the last variable is the depenendent variable.\n",
    "- Compute and print the mean squared error (MSE) in the predictions made by each linear regression model and the actual y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** What are the mean squared errors for each of the regression methods on the brain data? Which method(s) do best and which do the worst **and why**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been creating linear regression fits of form $y =c_0 + c_1x_1 + c_2x_2 + \\ldots$, where $x_i$ are independent variables (columns of $A$) and $c_i$ are corresponding coefficients in $c$. However, this equation only allows us to fit data with a line/plane. This may not be the best choice for all datasets.\n",
    "\n",
    "In this task, you will generalize the linear regression model form to include higher-degree (>1) polynomial terms and explore how this may improve fits to complex data. For example, assume we're doing a simple linear regression with independent variable $x_1$ and dependent variable $y$. A linear regression that fits data with a quadratic shape has the form$$y = c_0 + c_1x_1 + c_2x_1^2$$ \n",
    "\n",
    "Complete the following steps to add support for polynomial regression in your `LinearRegression` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Build the polynomial matrix of the independent variable\n",
    "\n",
    "The polynomial matrix contains the independent variable raised to a different power in each column. For example, if $A$ originally has a column vector for the independent variable $\\vec{x_1}$ ($A = [\\vec{x_1}]$) and we wanted to make the above quadratic model, we would append $x_1^2$ ($A = [\\vec{x_1}, \\vec{x_1^2}]$).\n",
    "\n",
    "**TODO:**\n",
    "Implement and test `LinearRegression::make_polynomial_matrix` (*helper method*) that takes care of raising the independent variable samples to different powers.\n",
    "\n",
    "##### Test `make_polynomial_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.r_[1:10].reshape((9, 1))\n",
    "test_p = 3\n",
    "\n",
    "# Test cubic\n",
    "lin_reg = linear_regression.LinearRegression(data.Data())\n",
    "print(f'Your polynomial matrix:\\n{lin_reg.make_polynomial_matrix(test_A, 3)}')\n",
    "\n",
    "true_mat = '''\n",
    "[[  1.   1.   1.]\n",
    " [  2.   4.   8.]\n",
    " [  3.   9.  27.]\n",
    " [  4.  16.  64.]\n",
    " [  5.  25. 125.]\n",
    " [  6.  36. 216.]\n",
    " [  7.  49. 343.]\n",
    " [  8.  64. 512.]\n",
    " [  9.  81. 729.]]\n",
    "'''\n",
    "print('It should look like:\\n', true_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Add support for polynomial regression\n",
    "\n",
    "This can be performed in 3 steps:\n",
    "\n",
    "1. Implement the `LinearRegression::polynomial_regression` method to perform the polynomial regression (*alternate option: you may instead update your `linear_regression` method. If you decide to go this route, add a keyword argument for the polynomial degree with a default value of 1 to preserve compatability with regular linear regression.*).\n",
    "    - I suggest using your QR solver, but you could use any of the ones you have implemented.\n",
    "2. Update `LinearRegression::predict`: Run `make_polynomial_matrix` on the \"A\" matrix that enters into the computation $y = Ac$ if `self.p > 1`.\n",
    "3. Add support for plotting polynomials in `LinearRegression::scatter` by generalizing the plotted regression line to a regression polynomial if `self.p > 1`:\n",
    "    - Getting your polynomial \"x\" values: Run `make_polynomial_matrix` on your evenly-spaced line sample points. To get the shapes to work out, you may need to add a trailing singleton dimension to your \"x\" sample points. For example, if you have 1000 \"x\" sample points, make the shape `(1000, 1)` rather than `(1000,)`.\n",
    "    - Getting your polynomial \"y\" values: Use matrix multiplication with your polynomial regression model slopes and/or intercepts.\n",
    "\n",
    "*There is no explicit test code here — visualizing the fit in the next subtask will help you debug!*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Run a polynomial regression\n",
    "\n",
    "In this subtask, you will debug your polynomial regression implementation and experiment fitting some data with it.\n",
    "\n",
    "#### Test: Polynomial regression with linear model ($p = 1$)\n",
    "\n",
    "- In the cell below, fit the `poly_data.csv` dataset using polynomial regression where the polynomial degree $p = 1$. Use your QR solver.\n",
    "- Use `scatter` to plot the results.\n",
    "- Print out the mean squared error.\n",
    "\n",
    "The plot created by running the below cell should \"look right\" to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: polynomial regression with high degree polynomials\n",
    "\n",
    "Repeat the steps from the $p = 1$ test above in the cell below, but this time try $p = 15$. \n",
    "\n",
    "*Your regression fit should not be a line!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Describe the fit compared to `p=1` — is it better or worse? Why?\n",
    "\n",
    "**Question 7:** Describe what happens visually and in terms of the $R^2$ and MSE values as you experiment with the polynomial degree between 1 and 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:** \n",
    "\n",
    "**Answer 7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Overfitting\n",
    "\n",
    "In this task, you will experiment with how polynomial regression generals to data not used to fit the regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Create fit and validation sets\n",
    "\n",
    "The `poly_data.csv` dataset has 100 samples ($N=100$). In the cell below, split these samples into two separate \"datasets\" and create 2 `Data` objects representing:\n",
    "- The first 50% of samples will be used to fit the regression model (i.e. run linear regression on these samples). We will call this the **fit set** (data used to fit the regression).\n",
    "- The second 50% of samples will be set aside and only used to check how well the fitted regression generalizes to new data. We will call this the **validation set**.\n",
    "\n",
    "The data samples are already shuffled.\n",
    "\n",
    "*Hint: There is a helpful `Data` method for paring down a dataset into a certain range of samples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the `Data` object storing the fit set should yield:\n",
    "\n",
    "    -------------------------------\n",
    "    data/polydata.csv (50x2)\n",
    "    Headers:\n",
    "    X\tY\n",
    "    -------------------------------\n",
    "    Showing first 5/50 rows.\n",
    "    2.147\t11.382\n",
    "    9.465\t1.034\n",
    "    4.52\t20.251\n",
    "    1.974\t2.89\n",
    "    -3.358\t-6.809\n",
    "\n",
    "    -------------------------------\n",
    "\n",
    "Printing the `Data` object storing the validation set should yield:\n",
    "\n",
    "    -------------------------------\n",
    "    data/polydata.csv (50x2)\n",
    "    Headers:\n",
    "    X\tY\n",
    "    -------------------------------\n",
    "    Showing first 5/50 rows.\n",
    "    -3.65\t-4.658\n",
    "    9.69\t-6.107\n",
    "    -21.986\t-17.271\n",
    "    -8.694\t-2.22\n",
    "    -15.536\t-25.608\n",
    "\n",
    "    -------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Check fit generalization on validation set\n",
    "\n",
    "Here is the process for checking how well your fitted linear regression model generalizes to the validation data:\n",
    "1. Create `Data` objects for both the fit and validation data sets (*as you have already done*).\n",
    "2. Run linear regression on the fit set.\n",
    "3. Create a new `LinearRegression` object associated with the validation data.\n",
    "4. Copy over the fitted slope and intercept coefficients associated with the fit set to the validation set `LinearRegression` object.\n",
    "5. Create a scatterplot by calling `scatter` to show the validation data and the regression curve that uses the coefficients fitted on the fit dataset (copied over in Step 4).\n",
    "6. Calculate and report fit statistics (e.g. MSE, $R^2$).\n",
    "\n",
    "Before doing this, implement the following methods to help you copy over the fitted slope, intercept and other data from your fit set `LinearRegression` object:\n",
    "\n",
    "- `get_fitted_slope`: return the fitted regression slopes.\n",
    "- `get_fitted_intercept`: return the fitted regression intercept.\n",
    "- `initialize(ind_vars, dep_var, slope, intercept, p)`: set fields based on passed in parameter values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check overfitting with $p = 15$ polynomial regression model\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "1. Fit a polynomial regression model with $p = 15$ on the fit set.\n",
    "2. Create 2 scatter plots:\n",
    "    - Showing the fit set and regression curve fitted to it.\n",
    "    - Showing the validation set and regression curve that uses the fitted coefficients to the **fit set**.\n",
    "3. Compute and print the MSE for both the fit and validation sets.\n",
    "\n",
    "Use the 6 step process above to guide you through the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Describe how the fitted coefficients obtained for the fit set generalize to the validation set. Do they do a good or bad job? Why?\n",
    "\n",
    "**Question 9:** Does the generalization improve or worsen for polynomial degrees < 15? Are Back up your observations with numbers (e.g. MSE).\n",
    "\n",
    "**Question 10:** Are there any values/ranges of polynomial degrees that generalize acceptably to the validation set? Why do you think so?\n",
    "\n",
    "**Question 11:** What happens when you increase the polynomial degree much above 15? Why do you think this happens? Back up your observations with numbers (e.g. MSE)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:** \n",
    "\n",
    "**Answer 9:**\n",
    "\n",
    "**Answer 10:** \n",
    "\n",
    "**Answer 11:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### a. Reminder: AI Policy\n",
    "\n",
    "The goal of extensions is to learn and create something new beyond the base project that excites you. To serve this goal and out of fairness to your hardworking classmates, **AI (e.g. ChatGPT, Copilot, etc.) should NOT be used in ANY way on this project and extensions.** This includes both written analysis, plotting, and code. We will only grade **your** work, not an AI's. **We will stop grading your project if we notice AI-generated content (to any capacity).**\n",
    "\n",
    "### b. Guidelines\n",
    "\n",
    "To receive credit for any extension, you must:\n",
    "1. Not modify / prevent any code from the core project from working (e.g. make a copy before changing). In other words, **the notebook test code should still work!**\n",
    "2. **You must describe what you did and what you found in detail**. This includes a summary of parameter values used in your simulations.\n",
    "3. Include (*labeled!*) plots and/or numbers to present your results.\n",
    "4. Write up your extensions below or in a separate notebook.\n",
    "5. Give kudos to all sources, including anyone that you consulted.\n",
    "\n",
    "### c. Suggestions\n",
    "\n",
    "**Rule of thumb: one deep, thorough extension is worth more than several quick, shallow extensions!**\n",
    "\n",
    "The ideas below are **suggested** extensions — feel free to go in another direction related to this project that is not listed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Your own data\n",
    "\n",
    "- Run linear regression on datasets that interest you. Identify your hypotheses about the association between variables and test them out. Make plots and report all relevant metrics fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear regression algorithm comparison\n",
    "\n",
    "- Compare the linear regression methods that you implemented on a dataset of your choice.\n",
    "- Research and implement matrix condition number. Find a dataset with a poor matrix condition number and then compare the regression methods. Which does best and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confidence intervals or other kinds of error bars on linear regression plots\n",
    "\n",
    "- Add the option to plot 95% confidence intervals on the linear regression predictions in your plot functions (e.g. `scatter`). [This website](https://real-statistics.com/regression/confidence-and-prediction-intervals/) should be a helpful reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Overfitting\n",
    "\n",
    "- Run polynomial regression on other datasets. What degree polynomial works well? When do you overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stepwise linear regression\n",
    "\n",
    "- Implement the stepwise linear regression discussed in class where you add variables to the regression model one-by-one in a greedy fashion: each variable added out of the available ones not already entered in the regression should result in the largest increase in the adjusted $R^2$ value on the validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
